# tinyllama_local
Text Generation model, hosting the tinyllama LLM model locally using ollama and accessing it via flask API in webpage 
